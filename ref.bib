% Dataset paper
@article{dataset,
    title = "Variation in body size and life-history traits in Drosophila aldrichi and D. buzzatii from a latitudinal cline in eastern Australia",
    author = "Volker Loeschcke and J{\o}rgen Bundgaard and Barker, {J S F}",
    year = "2000",
    language = "English",
    volume = "85",
    pages = "423--433",
    journal = "Heredity",
    issn = "0018-067X",
    publisher = "Nature Publishing Group",
}
% Collection of datasets paper
@article{datasetcollections,
    title = "A collection of Australian Drosophila datasets on climate adaptation and species distributions",
    author = "Sandra B. Hangartner, Ary A. Hoffmann, Ailie Smith and Philippa C. Griffin",
    year = "2015",
    language = "English",
    journal = "Sci Data",
    doi = "10.1038/sdata.2015.67",
    url = "https://doi.org/10.5061/dryad.k9c31",
}
% Feature importance - Gini importance, MDI
@misc{mdi,
    title = "Manual On Setting Up, Using, And Understanding Random Forests V3.1",
    author = "Leo Breiman",
    year = "2002",
}
% SML book
@book{smlbook,
   author = {Lindholm, Andreas and Wahlstr\"om, Niklas and Lindsten, Fredrik and Sch\"on, Thomas B.},
   year = 2022,
   title = {Machine Learning - A First Course for Engineers and Scientists},
   publisher = {Cambridge University Press},
   URL={https://smlbook.org},
}
% SVM visualisation
@misc{svmvis,
  title = {Plot classification boundaries with different SVM kernels},
  howpublished = {\url{https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html}},
  author = "Gaël Varoquaux",
}
% SVM kernel
@inproceedings{svmkernel,
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
title = {A training algorithm for optimal margin classifiers},
year = {1992},
isbn = {089791497X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/130385.130401},
doi = {10.1145/130385.130401},
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
pages = {144–152},
numpages = {9},
location = {Pittsburgh, Pennsylvania, USA},
series = {COLT '92}
}
